{"cells":[{"cell_type":"markdown","source":["その意気です！実際に手を動かすのが一番の近道です。\n","アップロードしていただいたColabのノートブック（`LoRA_Elith_ver1/2`）には、すでに **Unsloth** を使った最強の環境が整っています。\n","\n","せっかくなので、今回の案件である**「診察テキストの要約」**をテーマに、実際に小さな学習（LoRA）を回してみましょう。\n","\n","以下の手順で、お手元のColabを実行してみてください。\n","\n","---\n","\n","### ステップ1：モデルとデータの準備\n","\n","Colabのノートブックに、以下のコードブロックを追加・実行してください。ここでは、Qwenよりもさらに軽量で学習しやすい「Llama-3.2-1B」をベースにします。"],"metadata":{"id":"nj8eT2us2OIo"}},{"cell_type":"code","source":["# 一旦、既存のキャッシュによる衝突を防ぐためにこれらを試してください\n","!pip install --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps xformers trl peft accelerate bitsandbytes\n","# 1. unsloth_zoo をインストール\n","!pip install unsloth_zoo"],"metadata":{"collapsed":true,"id":"t7QoA2Jv2o-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","\n","# 1. モデルの読み込み（Llama-3.2-1Bを指定）\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Qwen2.5-3B-Instruct\",\n","    max_seq_length = 2048,\n","    load_in_4bit = True, # 4bit量子化でメモリ節約\n",")\n","\n","# 2. LoRAの設定（ここが「ランク」や「アルファ」の出番！）\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # ランク：16〜32くらいが標準\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","    lora_alpha = 32, # アルファ：ランクの2倍がセオリー\n","    lora_dropout = 0,\n","    bias = \"none\",\n",")"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"id":"zUEcNKvW2OIt"}},{"cell_type":"markdown","source":["---\n","\n","### ステップ2：医療用「要約」データの作成\n","\n","次に、モデルに「どういう風に要約してほしいか」を教えるためのデータ（インストラクション）を数件分、コードで作成します。"],"metadata":{"id":"PsQZzFhp2OIv"}},{"cell_type":"code","source":["from datasets import Dataset\n","\n","# 学習用データ（お手本を増やせば増やすほど賢くなります）\n","train_data = [\n","    {\n","        \"instruction\": \"次の診察記録を、重要な疾患名と処置方針に絞って簡潔に要約してください。\",\n","        \"input\": \"患者は50代女性。昨日からの激しい腹痛。検査の結果、急性虫垂炎と診断。即日手術を行い、経過観察中。\",\n","        \"output\": \"【疾患名】急性虫垂炎【処置方針】手術施行、経過観察。\"\n","    }\n","]\n","\n","# プロンプトの型を作る関数\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # モデルが理解しやすい形式に整える\n","        text = f\"### 指示:\\n{instruction}\\n\\n### 診察記録:\\n{input}\\n\\n### 要約:\\n{output}\"\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","\n","# --- ご指摘の重要パーツ ---\n","dataset = Dataset.from_list(train_data)\n","dataset = dataset.map(formatting_prompts_func, batched = True)"],"metadata":{"id":"yHFgkLt85e2C","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","### ステップ3：学習（Train）の実行\n","\n","いよいよ学習ボタンを押す感覚で実行します。"],"metadata":{"id":"6X92XW6e2OIv"}},{"cell_type":"code","source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset, # 本来はここでデータセットを渡します\n","    dataset_text_field = \"text\",\n","    max_seq_length = 2048,\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        max_steps = 60, # テストなので少なめに\n","        learning_rate = 2e-4, # 学習率\n","        fp16 = not torch.cuda.is_bf16_supported(),\n","        logging_steps = 1,\n","        output_dir = \"outputs\",\n","    ),\n",")\n","trainer.train()"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"id":"un75EYIv2OIw"}},{"cell_type":"code","source":["# 推論モードに切り替え\n","FastLanguageModel.for_inference(model)\n","\n","# テスト用の入力（1Bで失敗した時と同じ内容を推奨）\n","test_input = \"患者は50代女性。昨日からの激しい腹痛。検査の結果、急性虫垂炎と診断。即日手術を行い、経過観察中。\"\n","\n","inputs = tokenizer(\n","    [\n","        f\"### 指示:\\n次の診察記録を、重要な疾患名と処置方針に絞って簡潔に要約してください。\\n\\n### 診察記録:\\n{test_input}\\n\\n### 要約:\\n\"\n","    ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# 生成！\n","outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n","print(tokenizer.batch_decode(outputs)[0])"],"metadata":{"id":"oAhRHcLh66re","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","### ステップ4：学習したモデルを保存してOllamaへ！\n","\n","学習が終わったら、その「知能（LoRAアダプタ）」を保存します。Unslothの凄いところは、これを **GGUF形式（Ollamaで動かせる形式）** に一発で変換できるところです。"],"metadata":{"id":"Mdau1VuR2OIw"}},{"cell_type":"code","source":["import gc\n","import torch\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"2FQuKMkgcH5a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GGUF形式で保存（これがレッツノートのOllamaで動く！）\n","model.save_pretrained_gguf(\"model_lora_medical\", tokenizer, quantization_method = \"q4_k_m\")"],"outputs":[],"execution_count":null,"metadata":{"id":"9YfrQtwf2OIx","collapsed":true}}],"metadata":{"colab":{"provenance":[{"file_id":"1TSfcIvxSm58A9lyg8HSFoCvvbMIaSAGa","timestamp":1766194439927}],"gpuType":"L4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}